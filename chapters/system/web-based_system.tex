\chapter{Web-Based Responsive Spoken Dialogue System}
\label{chap:web-based_responsive_spoken_dialogue_system}

\lettrine{I}{ntroduction} to this chapter\ldots
% complete, web-based \ac{sds} with focus on vocal adaptation.

\pagebreak

\section{Overview and key aspects}
\label{sec:overview_and_key_aspects}

Simulating and triggering accommodation effects occurring in \ac{hhi} like those found in \cref{chap:conv_analysis} in \acp{sds} takes them one step further toward human-like communication.
The system presented in this chapter encapsulates the knowledge acquired from the experiments in \cref{part:experiments}, the behavior designs developed in \cref{part:modeling}, and the module introduced in \cref{chap:convergence_module_for_sdss} which enables the vocal changes.
It contains mechanisms to track the states and changes of segment-level and suprasegmental-level phonetic features during a dialogue.
All the analyses are automated and run in real-time, which not only saves a lot of time and manual work typically needed in convergence studies, but also makes the system more suitable for integration into other applications.
It holds the following key principles:
%
\begin{description}
	\item[Focus on adaptation] --
	the main goal of the system is to offer a tool for investigating vocal accommodation in \ac{hci} for both online experiments and offline analyses.
	Putting vocal accommodation under the spotlight is the core novel contribution of the system, as very few systems offer such capabilities at all, and with control over the accommodative behavior in particular.
	
	\item[Customizability] --
	the system includes several components that can be modified, either for changing the accommodation behavior itself (features, parameters, etc.) or changing the setting (e.g., for different experiments).
	This allows experimenting with different scenarios and configurations and easily compare them in a controlled, reproducible environment.
	
	\item[Online scalability] --
	the system can run in a web browser without any installations or additional files\footnote{some features need to be enabled in the browser, like JavaScript and microphone access.
	However, any modern browser should not have any problem supporting all the necessary requirements.
	To increase performance, all speech analyses and processing are done on the server side.}.
	Since the system itself runs on a single server, it is also possible to operate multiple instances, each with its own configurations and parameters.
	This makes it easy to distribute, e.g., for remotely conducting an experiment where each participant may receive a different configuration.
\end{description}
%
Ultimately, this customizable system could help to deepen the experimental possibilities and automating some aspects of the processes convergence-related experiments typically comprises.
The system's architecture, and \ac{gui}, and functionality are described in \cref{sec:architecture,sec:online_and_offline_paths}.
The experiment presented in \cref{chap:shadowing_experiment_with_natural_and_synthetic_voices} is replicated in \cref{sec:showcase} to demonstrate the system's utilization.

\section{Architecture}
\label{sec:architecture}

As the system aims to offer a customizable playground for experimenting and studying phonetic adaptation in \ac{hci}, a key aspect of its architecture is the separation between client-side, server-side, and external resources (see \cref{fig:web-based_architecture}).
This separation makes it possible to run multiple clients on different machines at the same time with a single server collecting the data from all of them at the same time.
The server, ideally running on a dedicated machine, is operated by an a person responsible of designing and configuring the interactions, e.g., an experimenter.
It collects information and audio recordings from all interactions with the system (which can be deleted afterwards for privacy purposes).
This separation of the server grants the experimenter a lot of freedom and flexibility, since resources like feature configurations and dialogue domain can be modified independently of specific machines interacting with the system.
Additionally, multiple configurations can be prepared in advance (e.g., for different participant groups), regardless of the device the experiment will be performed on and before summoning the participants.
Configurations can even be changed mid-interaction.
These configurations are transparent to the users, and no effort is required from them (aside from start a new interaction in the case of some specific configurations).
This flexibility makes it easier and quicker to create new scenarios of interaction and to experiment with different features and parameters.

In addition to the technical advantages, letting users interact with the system on a separate machine broadens the usage possibilities.
For example, an experiment can be carried out remotely, without the need to invite participants to the recording studio one by one.
Furthermore, as the connection to the server is done via a web browser, participants can connect use the system with their own computers wherever and whenever it suits them, without any additional installation or technical configurations.
All of these makes it possible to collect data from many users rapidly and easily.
%
\begin{figure}[t]
	\centering
	\includegraphics[width=\linewidth]{web-based_architecture_no-ajax}
	\caption[Architecture of a web-based responsive spoken dialogue system]
		{The architecture of the web-based responsive spoken dialogue system.
		The background colors distinguish between client components, server components, and customizable external resources.
		The dashed line indicates that the feature predictions may or may not be passed from the model to the system depending on the feature definition and update parameter.}
	\label{fig:web-based_architecture}
\end{figure}
%
As shown in \cref{fig:web-based_architecture}, the main components of the system are the \ac{sds} itself (including the accommodation module; \cref{subsec:dialogue_system}), the \ac{gui} (\cref{subsec:graphical_user_interface}), and the external resources and configuration (\cref{subsec:models_and_cusomizations}).

\subsection{Dialogue system}
\label{subsec:dialogue_system}

The core of the system is the dialogue system component (see \cref{fig:web-based_architecture}), which controls the flow of the interaction, processes user's inputs, and generates the system's responses.
It uses the extended architecture presented in \cref{subsec:extended_sds}, which consists of typical \ac{sds} components such as \ac{nlu} and a \ac{dm}, but also contains the \ac{asp}  module that adds accommodation support \citep{Raveh2017SemDial}.
The implementation of this module in the system is as described in \cref{fig:adaptation_module_architecture}.
While the \ac{nlu} component uses merely the transcription provided by the \ac{asr}, the \ac{asp} module analyzes the speech signal itself.
Concretely, it tracks occurrences of the defined features and passes their measured values to the convergence model, as explained in \cref{subsubsec:tracked_features}, which, in turn, forwards the tracked feature parameters to the \ac{tts} synthesis component.
The \ac{tts} engine then takes the text generated by the \ac{nlg} component, and, if phonetic-level manipulation is supported, synthesizes the utterance using the values specified by the convergence model.
The connection between the dialogue system's modules is managed by the \emph{OpenDial} framework \citep{Lison2016opendial, Lison2015developing}.
The \ac{asr} module uses CMUSphinx \citep{Lamere2003sphinx} with additional customized functionality for obtaining the phonetic information required for the \ac{asp} module, and the \ac{tts} is driven by MaryTTS \citep{LeMaguer2017uprooted, Schroeder2003mary}.
The \ac{nlu} and \ac{nlg} modules are built using an OpenDial's domain file, as described in \cref{subsubsec:dialogue_domain}.

\subsection{\Acl{gui}}
\label{subsec:graphical_user_interface}

The user interacts with the system via an in-browser \ac{gui} (see \cref{fig:gui}).
At the top of the screen is a control bar, which offers the user overview and easy access to some main functions.
On the left side of the bar, the user can view the list of the interaction's turn history and jump to any of them.
It is also possible to see the list of tracked features and their current state.
Both lists can be reset using the Reset button (in red), which starts a new interaction using the current configurations (which may be changed before pressing the button).
On the other side of the bar, there are buttons for viewing on-screen how-to-use information and changing the settings of the system.
The rest of the \ac{gui} is divided into four areas:
A chat area, where the dialogue is shown,
an interaction area where the user provides input to the systems,
a plot area with interactive dynamic visualization of the tracked features,
and a notification area where prompts for the user can be shown.
The functionality of each is described in \crefrange{subsubsec:chat_area}{subsubsec:notification_area}

\subsubsection{Chat area}
\label{subsubsec:chat_area}

The interaction between the user and the system is shown in a chat-like format at the upper left part of the screen.
Each turn's utterance appears inside a chat bubble with different colors representing the two interlocutors, the user and the system.
The bubbles always contain single utterances, even if the other interlocutor has not spoken between them.
A turn can be replayed at any time using the Play button next to the turn number, corresponding to the turn order on the list accessible from the control bar.
Beside utterance bubbles, the system can also display general-purpose messages in this area, which do not progress the dialogue flow and do count as system utterances.
These messages can be used,
%
\begin{landscape}
	\begin{figure}[t]
		\centering
		\vspace*{-2cm}
		\hspace*{-2cm}
		\includegraphics[width=1.6\textwidth]{gui}
		\caption[Web system in-browser \acs{gui}]
			{}
		\label{fig:gui}
	\end{figure}
\end{landscape}
\noindent
for example, to give the user additional information or instructions regarding the interaction.

\subsubsection{Interaction area}
\label{subsubsec:interaction_area}

The user can interact with the system either with either written or spoken input using the controls at the bottom left part of the screen.
Spoken input can be provided either by speaking live into the microphone or via audio files with pre-recorded speech.
These are typically useful for online and offline usage, respectively (\cref{sec:online_and_offline_paths}), but pre-recorded utterances can also be useful for or reproducing previous experiments or comparing different accommodation configurations with the exact same user input.
Text-based interactions progress through the dialogue (if applicable) and trigger any subsequent domain model, but will not affect the tracked features, as no vocal input was provided.
This can be useful for quickly going through specific parts of an experiment (e.g., instructions or setup) or for continuing the dialogue without changing the system's representation of the tracked features.

\subsubsection{Plot area}
\label{subsubsec:plot_area}

\begin{figure}[t]
	\centering
	\includegraphics[width=\linewidth]{plot_area}
	\caption[Real-time dynamic visualization of phonetic changes]
		{The plot area showing the states of the feature \textipa{[E:]}~vs.~\textipa{[e:]} during an interaction.
		The system's (orange, bottom right) gradually adapts to the user's (blue, upper left) detected realizations.
		A prediction of the feature's current realization is given for both interlocutors.
		The text box shows the mouse-over annotation of the turn in which the system's realization changes vowel category.}
	\label{fig:plot}
\end{figure}

Visualization of the tracked features' changes over the course of the interaction are displayed in the upper right part of the screen.
Each feature is visualized separately, and new datapoints are dynamically added whenever applicable.
The type of a feature's plot can also change based on its characteristics, e.g., bars for one-dimensional features and lined scatter plots for two-dimensional features.
These plots are generated using the Plotly library\footnote{\url{https://plot.ly}}, which provides some interactive functionalities.
Hovering over a datapoint in the plot reveals additional information, such as the turn in which it was added, or the realized variant of the feature produced in that turn as predicted by its classifier.
\Cref{fig:plot} shows an example of such plot with several accumulated datapoints.

\subsubsection{Notification area}
\label{subsubsec:notification_area}

Whenever a message outside the content of the interaction needs to reach the user, it can be shown at the bottom right part of the screen.
Such messages may include indications of the system's activity, e.g., successful initialization of the interaction, warning and errors while uploading files, or any other prompt the experimenter might want the user to see, like additional instructions to consider during the experiment.
However, the latter is better achieved using the non-speaker turns in the chat area.
The notifications can be colored blue, green, orange, and red to differentiate different types of messages.

%\subsubsection{Settings and help}
%\label{subsubsec:settings_and_help}
%
%An additional modal window can be called, in which various settings can be changed, and some usage information is provided.
%Configurable settings include the convergence model parameters, domain file, \ac{gui} tweaks, and more.
%These settings can be modified at any point during the interaction, so that it is possible to experiment with different configurations in real-time.
%For persistent changes, it is also possible to edit the configuration file itself, which is loaded when the system starts.
%The usage tab explains the various functionalities of the system and how each area of the \ac{gui} works.
%It also lists special commands that can be executed from text field (used for simulating the user's input, as mentioned in \cref{subsubsec:interaction_area}).
%These commands include printing a short or detailed summary of the phonetic changes throughout the interaction, extracting the data from the features' plots (e.g., for further analysis), and more.
%\todo{screenshot with the settings tab}

\subsection{Models and customizations}
\label{subsec:models_and_cusomizations}

The system aims to offer a platform for \acp{sds} with convergence support that can be modified and customized according to the user's needs.
All of the aforementioned system components can be customized to some extent.
This also includes the phonetic convergence model, the features tracked by the system, and the dialogue domain.

\subsubsection{Tracked features}
\label{subsubsec:tracked_features}

The accommodation process is initiated by the phonetic features defined in the configuration file.
There are phonetic features that are prone to variation, and are triggered whenever the \ac{asr} component detects a segment containing a phoneme associated with one or more of these features.
The feature definitions may capture general tendencies or specific phonological rules, like schwa elision in German (see \cref{eq:elision_rule}).
As explained in \cref{subsec:computational_model}, each feature is detected and filtered based on its definition.
This definition can be easily changed to experiment with different accommodation effects.

\subsubsection{Dialogue domain}
\label{subsubsec:dialogue_domain}

The dialogue's flow is specified using OpenDial's XML-based format\footnote{\url{http://www.opendial-toolkit.net/user-manual/dialogue-domains}}.
This format offers a structure for building models, rules, and conditions, which define the \ac{dm} logic.
The rules connect between intents provided by the \ac{nlu} module to textual output generated by the \ac{nlg} module.
Additional parameters are introduced for triggering processing for other modules of the \ac{sds}, like the \ac{asp} module in the system discussed here.
More details about building a domain file can be found in \citet{Lison2016opendial}.
The format of the domain file makes it easy to define new scenarios for the system, like different experiment scenarios.
Rules are written mostly using regular expressions, which makes it possible for non-technical users to design most of the system's logic.
Since the \ac{dm} keeps track of parameters from all modules, the system's output can even be influenced by the state of the accommodation state in the \ac{asp} module (if such information is provided).

\subsubsection{Speech processing}
\label{subsubsec:speech_processing}

Multiple components of the system deal with different aspects of speech processing.
As each module in the system can be replaced independently, different engines and models can be used.
For example, the \ac{asr} engine can be replaced for improving performance or adding support for more languages (that is, all time the same functionality is offered by that engine and the phonemeset used by the system is updated accordingly).
The \ac{tts} component can be replaced as well, e.g., for changing the voice of the system.
Importantly, also the tool used for the phonetic analysis can be changed to improve accuracy or performance.
The models and tools described here are those that were used specifically for the showcase presented in \cref{sec:showcase}.
The \ac{asr} component uses CMUSphinx\footnote{Sphinx4 version 5prealpha, \url{https://cmusphinx.github.io/}} \citep{Lamere2003sphinx}, with an extension to the phoneme emission functionality to provide the \ac{asp} module the input it needs (see \cref{subsec:computational_model}).
The acoustic model and pronunciation dictionary were taken from online CMUSphinx models\footnote{\url{https://sourceforge.net/projects/cmusphinx/files/Acoustic\%20and\%20Language\%20Models/German/}}.
A customized \ac{asr} language model was created with SRILM \citep{Stolcke2002SRILM}).
All the segmental and suprasegmental analyses required for the measuring accommodation were done using Praat \citep{Boersma2018praat}.
MaryTTS \citep{LeMaguer2017uprooted} was used as the \ac{tts} engine of the system, with \texttt{bits1-hsmm} and \texttt{bits3-hsmm} for its female and male voices, respectively.

\section{Online and Offline paths}
\label{sec:online_and_offline_paths}

\missingfigure{online and offline paths}

\section{Showcase: simulating a shadowing experiment}
\label{sec:showcase}

As a showcase of the system capabilities, it was utilized to replicate the shadowing experiment described in \cref{chap:shadowing_experiment_with_natural_and_synthetic_voices}.
The experiment is designed to trigger and analyze phonetic convergence by confronting the participants with stimuli, in which certain phonetic features are realized in a way different from their own.
This was done using the offline path of the system, to can simulate a real experiment and automate certain parts of it that would otherwise be performed manually.
The replication used the original stimuli and utterances of one of the participants.
However, analyses originally done post facto (and to different extents manually), like detecting the realized variant, measuring the features' values, etc., were now done automatically.
This demonstrates an automated, reproducible execution, and also offers additional insights via classification of feature realizations and dynamic visualizations in the web \ac{gui} (\cref{subsec:graphical_user_interface}).
Finally, using the system, the experiments becomes dialogue-based rather than a mere experimental setting, which enhances its \ac{hci} nature.

\subsection{Target features and stimuli}
\label{subsec:target_features_and_stimuli}

For the experiment simulation, two of the three features in the original experiment (\cref{chap:shadowing_experiment_with_natural_and_synthetic_voices}) were used.
In addition to the \textipa{@}-length featured shown in \cref{subsec:computational_model}, the showcase in \cref{sec:showcase} uses also the following definition for the feature \textipa{[E:]}~vs.~\textipa{[e:]} (see \cref{subsec:target_features_HCIConv}):
%
%\begin{description}[labelindent=1.5cm, labelwidth=\widthof{\quad \bfseries calculation}]
%	\item[name]	ee\_E
%	\item[phoneme] EHH
%	\item[context] .* EHH .*
%	\item[initial] 451, 2116, 2763
%	\item[minimum] 300, 1500, 2500
%	\item[maximum] 750, 2900, 4800
%	\item[measure] formants
%	\item[calculation] decaying average
%	\item[sensitivity] 0.3
%\end{description}
%
\begin{Verbatim}[tabsize=4, commandchars=\\\{\}]
	- \textbf{`e\_E\_vowel'}:
			\textbf{phoneme}: EHH
			\textbf{context}: '.* EHH .*'
			\textbf{initial}: 450, 2100
			\textbf{minimum}: 300, 1500
			\textbf{maximum}: 750, 2900
			\textbf{measure}: formants
			\textbf{calculation}: decaying average
			\textbf{sensitivity}: 0.3
\end{Verbatim}
%
%The value of the key \emph{measure} is \enquote{formants}, which means that this feature is evaluated by the segment's formant values (as specified in the corresponding signal processing script).
%The values of the \emph{minimum} and \emph{maximum} keys stand for the acceptable value range for this feature.
%This avoids distorted values due to \ac{asr} error and lets the user put their phonetic expertise to use.
\noindent
The three values of these keys set the first two formant frequencies.
The calculation method for this feature is decaying average, which is similar to the regular average but with each value contributing exponentially less to the final value, so that the last (newest) exemplar contributes the most.
Adding such property to the measure gives more weight to new exemplars that were received chronologically closer to the current turn and thus makes the change more strongly influenced by the productions closer to the accommodation change.
Using this measure comes to support the analogy of the exemplar pool to short-term memory, which remembers recent event better than older ones.
Decaying average is defined here as
%
\begin{equation} 
	\label{eq:decaying_average} 
	\mu_n = \frac{1}{n}\sum_{i = 2}^{n}(\eta v_i + (1 - \eta )\mu_{i-1}), 
\end{equation} 
\eqname{Decaying average}
%
\noindent
where $n$ is the number of exemplars in memory, $\eta$ is the decay rate (here, 0.5), $\mu_{i-1}$ is the accumulated decaying average from the previous exemplar, and $v_i$ is the value of the $i$-th exemplar. 

%\begin{description}[labelindent=1.3cm, labelwidth=\widthof{\quad \textipa{[I\c{c}]}~vs.~\textipa{[Ik]}}]
%	\item [\textipa{[E:]}~vs.~\textipa{[e:]}] in word-medial $\langle$ä$\rangle$
%	\item [\textipa{[I\c{c}]}~vs.~\textipa{[Ik]}] in word-final $\langle$-ig$\rangle$
%	\item [\textipa{[\s{n}]}~vs.~\textipa{[@n]}] in word-final $\langle$-en$\rangle$
%\end{description}
%\noindent

As in the original experiment, the stimuli consist of these features embedded into 15 short carrier sentences and 25 filler sentences, in which none of the features occur.
Although the features' underlying values are gradual, they are perceived as two-way categorical variations.
To map these underlying values to a specific variant, the system associates a classifier with each feature, as explained in \cref{subsec:classifiers_training}.
\cref{tab:target_features} shows an example for each feature (see \cref{app:shadow_experiment_stimul} for more examples).
%
\begin{table}[t]
	\centering
	\begin{tabularx}{\linewidth}{@{}*{5}{l}}
		\toprule
		
		War          	& das          			& Ger\textbf{\underline{ä}}t	& sehr          	& teuer? \\
		\emph{Was} 		& \emph{the} 			& \emph{device}           		& \emph{very} 		& \emph{expensive?} \\[0.1cm]
		
%		Ich          	& bin         			& sücht\textbf{\underline{ig}}	& nach				& Schokolade. \\
%		\emph{I}   		& \emph{am} 			& \emph{addicted}				& \emph{to} 		& \emph{chocolate.} \\[0.1cm]
		
		Wir         	& besuch\textbf{\underline{en}} 						& euch				& bald          & wieder. \\
		\emph{We} 		& \emph{will visit}		& \emph{you} 					& \emph{soon} 		& \emph{again.} \\
		\bottomrule
	\end{tabularx}
	\caption[Example sentence for selected phonetic features]{Examples of stimuli containing the target features. Each stimulus contains only one feature.}
	\label{tab:target_features}
\end{table}

\subsection{Experimental procedure}
\label{subsec:experimental_procedure}

% put this entence somewhere
The domain file use in \cref{sec:showcase} was designed to replicate the role of the experimenter in \cref{subsubsec:procedure_hci}, i.e., presenting the next stimulus to the participant in a semi-randomized order.
%%%%%%%%%%%%%

Only those parts of the experiment are explained here that are relevant to its simulation in the dialogue system.
For validation purposes, the experimental procedure stayed as faithful as possible to the procedure of the original experiment, even though more aspects of it could be automated.
The entire experimental procedure, as well as more information regarding its setup, is detailed in \citet{Gessinger2017Interspeech}.

The experiment consists of three phases: \emph{baseline} production, \emph{shadowing} task, and \emph{post} production (see \cref{fig:HCIConvFlow}).
In the \emph{baseline} phase, the participants were asked to read out the stimuli from a monitor.
The participant's most frequent variant in this phase is configured into the system.
Then, in the \emph{shadowing} task, the participants produced the stimuli sequentially, each after listening to another voice (either natural or synthetic, both male and female) producing the opposite category of the relevant target feature.
Based on the production in this phase, the participant's tendency, pace, and degree of convergence were analyzed.
Finally, in the \emph{post} phase, the participant once again read out the stimuli from a screen.
The purpose of this phase was to examine whether any convergence remained in effect even in the absence of non-preferred input.
A shortened example of the shadowing phase's flow is shown in \cref{app:dialogue_example}.

\subsection{Classifiers training}
\label{subsec:classifiers_training}

% text from feature definition:
\todo[inline]{need to put a sentence pointing here where the feature definitions are described?}
\todo[inline]{somewhere say the real-time classification saves the time the experimenter needed to judge the participants' productions either during the experiment or afterwards. In other applications, it can also be used to provide feedback while the interaction is still going.}

Since the system's main focus is on phonetic convergence, it is essential to also provide useful information regarding the realizations of the tracked features.
To that end, a classifier can be associated with each feature to provide real-time predictions for both the user's and the system's realizations of that features, as demonstrated in \cref{fig:plot}.
With this information available, more meaningful insights can be gained into the variation dynamics in the dialogue.

The validation stays faithful to the original experiment in every possible aspect.
Therefore, the training data used for each classifier contains only the productions of the corresponding target feature from a single stimulus set, since these are the productions to which the participants were exposed during the experiment.
This provides relatively few -- but at the same time very precise -- datapoints for each classifier,
which were obtained using the same signal processing technique as the data collected in the experiment.
The classifiers were trained offline on these datapoints.
However, the system also supports incremental, online re-training whenever requested by the user,
such as, every time the convergence model is updated.

A \ac{smo} \citep{Platt1999fast, Platt1998sequential} implementation of the \ac{svm} classifier \citep{Vapnik1998support} was using for training.
Depending on the target feature, this may also be a multivariate \ac{svm} classification \citep[e.g.,][]{Joachims2005support}.
Each turn's predictions, as well as other details, are added as interactive annotations to the dynamic plot of the relevant features, as shown in \cref{fig:plot}.

\subsection{Validation}
\label{subsec:validation}

\begin{figure}[t]
	\centering
%	\includegraphics[width=\linewidth]{web-based_architecture}
	\missingfigure{bull's eye figure and bars figure}
	\caption[]
		{}
	\label{fig:validation}
\end{figure}

The shadowing experiment was simulated by configuring the domain file with a definition of the transition between the phases, as well as the flow within each phase.
This automates the procedure and adapts it to the participant's pace.
Additional variables are defined and handled as well, helping the system to track the experiment's flow and state.
Participants were simulated by using their recorded speech from the original experiment.
After each turn, any relevant \ac{sds} module was triggered based on the simulated participant's input.
The stimulus order from the original experiment was preserved.
Since the \emph{post} phase is effectively the same as the \emph{baseline} phase, it was considered redundant and excluded from the simulation.
The validation for the feature \textipa{[E:]}~vs.~\textipa{[e:]} is shown here as a representative example for the phonetic adaptation capability of the system.

For the baseline phase, the validation examined the degree to which the underlying convergence model accumulated enough data to adopt the user's variant of the feature.
Stronger and quicker adoption indicates a more stable and precise preferred variant of the participant.
The user's and model's preferred variants were determined based on the majority vote at the end of this phase.
For example, if the user realized one variant twice and another three times, the latter is considered to be preferred.
\Cref{tab:validation_baseline} shows the degree of the model's adoption of the user's preferred variant, according to their majority votes, using different values of the \emph{sensitivity} parameter.
Interestingly, higher values do not necessarily result in higher percentages.
The value 0.3 provided the highest results, and was therefore used through the rest of the simulation.

\begin{table}[t]
	\centering
	\begin{tabularx}{\linewidth}{X*{4}{S[table-format=2.1]}}
		\toprule
		sensitivity (\numrange{0}{1}) &  0.2 &  0.3 &  0.4 &  0.5 \\
		adoption (\si{\percent})      & 79   & 86   & 75   & 69   \\
		\bottomrule
	\end{tabularx}
	\caption{Model's convergence coverage with different parameters.}
	\label{tab:validation_baseline}
\end{table}

After obtaining the preference of each participant, the degree of convergence was examined per utterance in the shadowing phase.
The participants were grouped based on their convergence behavior:
One group of participants showing low to no tendency to converge (\SI{\le 10}{\percent} of their utterances),
the second, with varying degrees of convergence (\SIrange{10}{90}{\percent}),
and a third group of participants who were very sensitive to the stimuli's variation (\SI{\ge 90}{\percent} of their utterances).
These groups are labeled \emph{Low} (\SI{23}{\percent} of participants), \emph{Mid} (\SI{50}{\percent}), and \emph{High} (\SI{27}{\percent}), respectively.
The feature's classifier was determined on the fly, so that the prediction for each utterance was decided based on the stimulus type to which the participant was listening (e.g., a classifier trained on synthetic stimuli was used for participants listening to such stimuli).

For the purpose of the validation, the shadowing phase is evaluated as an annotation task of the realized variation in the utterances.
The three annotation sets are the stimuli themselves (\emph{Stim}), the system's online classification of the participants' production (\emph{Sys}), and the labels obtained as additional references for the participants' productions (\emph{Ref}).
Note that \SI{100}{\percent} would mean complete convergence to every stimulus, which cannot be reasonably expected \citep[cf.][]{Gessinger2017Interspeech}.
The same holds for the Cohen's kappa ($\kappa$) values\footnote{as calculated by the \emph{kappa2} command of the \emph{irr} R package, v0.84, \url{https://cran.r-project.org/package=irr}}, which are expected to be lower for \emph{Low}, as a lower degree of convergence was found among these participants.
As \cref{tab:validation_shadow_similarity} shows, convergence was found in \SI{48}{\percent} of the utterances for \emph{Sys-Stim} and \emph{Ref-Stim}.
However, the \emph{Ref-Sys} similarity is only \SI{66}{\percent}, which means that convergence was found in different instances.
The $\kappa$ values in \cref{tab:validation_shadow_kappa} give an additional view on these results.
The \emph{Ref-Sys} agreement is around 0.2 (fair agreement) for \emph{Mid}, but much lower for the other two groups,
confirming that more differences are expected to be found between \emph{Sys-Stim} and \emph{Ref-Stim} in the \emph{Low} and \emph{High} groups.
Moreover, \emph{Sys-Stim} shows greater variation of $\kappa$ values across the three groups, indicating higher separation between the three categories of convergence behavior.
This provides a more meaningful overview of the participants' convergence patterns.
% kappa itepratation classes taken from http://www.statisticshowto.com/cohens-kappa-statistic/

\section{Conclusion and future work}
\label{sec:conclusion}

We have introduced a \acf{sds} with phonetic convergence capabilities.
The system is able to track the state of configured phonetic features and change its \acf{tts} output accordingly, based on an internal convergence model.
This combines work done in the fields of phonetic convergence and adaptive \acp{sds}.
Many aspects of the system are customizable, which makes it flexible in terms of possible supported scenarios.
This includes multiple parameters defining the target phonetic features, which allows experimentation with different features.
The system can run on a separate server, which makes it easier to scale its use.

In addition, we replicated a shadowing experiment, which examined phonetic convergence regarding certain features, showcasing the \ac{sds}'s performance and simulation capabilities.
Running the experiment in this way not only saved time by automating the annotation and phonetic analysis, but also offered additional insight such as visualization and on-the-fly classification.

By validating the system in this way, we are confident that phonetic convergence can be studied using our \ac{sds} in a more objective way, focused on \ac{hci}.
We firmly believe that this is one step forward toward personalized, phonetically aware \acp{sds}, which are likely to enable more natural and efficient interaction.

\begin{table}[t]
	\centering
		\begin{tabularx}{\linewidth}{X*{3}{S[table-format=2.0]}}
			\toprule
			Group & {\emph{Sys}-\emph{Stim}} & {\emph{Ref}-\emph{Stim}} & {\emph{Ref}-\emph{Sys}} \\
			\midrule
			\emph{Low}  & {<1} &  7 & 16 \\
			\emph{Mid}  &  22  & 23 & 32 \\
			\emph{High} &  26  & 18 & 18 \\
			All   		&  48  & 48 & 66 \\
			\bottomrule
		\end{tabularx}
%		\caption{Similarity (\si{\percent})}
		\label{tab:validation_shadow_similarity}
\end{table}
\begin{table}
		\begin{tabularx}{\linewidth}{X*{3}{S[table-format=2.2]}@{\quad}}
			\toprule
			Group & {\emph{Sys}-\emph{Stim}} & {\emph{Ref}-\emph{Stim}} & {\emph{Ref}-\emph{Sys}} \\
			\midrule
			\emph{Low}  & -0.57*** & -0.08   & 0.17    \\
			\emph{Mid}  & -0.15*   & -0.15*  & 0.27*** \\
			\emph{High} &  0.81*** & -0.04   & 0.03    \\
			All   		& -0.11*   & -0.13** & 0.21*** \\
			\bottomrule
		\end{tabularx}
%		\caption{Agreement (Cohen's $\kappa$).
%			\enquote*{*} means $p$~value $<0.05$, \enquote*{**} means $p$~value $<0.005$, and \enquote*{***} means $p$~value $<0.0005$}
		\label{tab:validation_shadow_kappa}
	\caption[Similarity and agreement evaluation of system and stimulus sets]{A summary of the similarity and agreement between the system's (\emph{Sys}), references (\emph{Ref}), and stimuli (\emph{Stim}) annotations of the shadowing phase productions.}
	\label{tab:validation_shadow}
\end{table}
	\todo{somehow combine these tables. or maybe replace it with a better analysis}

Future work will pursue two independent directions:
regarding phonetic convergence, supporting more features will make the system more comprehensive and useful for studying a wider range of phenomena.
Specifically, adding support for supra-segmental (i.e., prosodic) features will enable replication of experiments similar to, e.g., \citet{Levitan2014acoustic, Levitan2016implementing} in the same manner as in \cref{sec:showcase}.

Regarding user acceptance, it would be interesting to examine whether users show any preference toward an \ac{sds} that converges to their speech on the phonetic level, and whether they would change their speaking style based on the system's output, forming an interaction with mutual and dynamic convergence.
The first research question can be tested by comparing user interaction with a baseline system and one with convergence capabilities, and evaluating the users' performance and satisfaction.
The second research question can be investigated by comparing the users' speech when interacting with either system configuration.
Additionally, to test the system's influence on users' speech, the users can train with an intelligent \acf{call} or \acf{capt} system, which will change its learner model based on their input.
Task completion rate, performance accuracy, and completion time metrics can be used to evaluate how helpful the system is.

\begin{figure}[h!]
	\centering
	\includegraphics[width=\linewidth]{pipeline}
	\caption[Phonetic convergence algorithm pipeline]{Overview of the phonetic convergence pipeline used in the computational model.
		Rectangles represent steps where an action is performed, round rectangles are inputs (either for external resources or from the system), and diamonds stand for decision points.
		When a decision node does not have a \enquote{no} outcome it means that the process is terminated (and therefore no convergence occurs) if the condition is not met.
		The pipeline can only be successfully completed at the \enquote{Set feature's new value} node.
		However, if the \enquote{Add exemplar} action was performed prior to termination, the exemplar is not removed and will be taken into consideration the next time the pipeline is triggered for the feature with which it is associated.
		The \enquote{feature definitions} come from the configuration file and can be changed by the user.}
	\label{fig:adaptation_module_pipeline}
	\todo{fix in figure: two ``yes''es are going out of the Convergence Limited block}
	\todo{use colors to associate each shape (excluding arrows) to a step in the pipeline (maybe same colors as in stripes figure?)}
\end{figure}

\todo[inline]{somewhere in the thesis where evaluation of an actual system is discussed, talk in detail about the challenge that there is no absolute correct answer regarding how and how much to converge (user dependent, etc.) and for the same reasons there is no gold standard to compare against. so need to find other criteria for evaluation.}

\section{Dialogue example}
\label{app:dialogue_example}
\todo{Put a relevant and interesting chat example (if that's even necessary here). or make several examples and put them all in an appendix}
%\begin{figure}[h!]
%	\centering
%	\adjustbox{max width=\linewidth}{\input{figures/chat_example.tikz}}
%	\caption{An illustration of the chat area at the end of the shadowing task.
%		The sentences shown here are the subset of sentences containing the feature \textipa{[E:]}~vs.~\textipa{[e:]}.
%		User utterances are in blue, the system's are in orange.
%		The interaction starts with the system asking the user whether he is read, where only a \enquote{yes} will progress forward.
%		The user then repeats the sentences presented by the system, until the system declares that there are no sentences left.
%		Each utterance is labeled with the turn number it appeared in.
%		This numbering can be used to track the analysis of this turn, as shown in \cref{fig:plot}.}
%	\label{fig:chat_example}
%\end{figure}
