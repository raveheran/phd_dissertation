\section{Speech variations in human-human-computer interaction}
\label{sec:speech_variations_in_hhci}

% *** Alexa ESSV paper ***

\section{abstract}

This paper presents a study that examines the difference of certain phonetic features between \acf{hds} and \acf{dds} in \acfp{hhci}.
The corpus used for the analyses consists of tasks performed by participants in cooperation with a human confederate and/or a computer-based interlocutor.
This includes distributional and temporal analyses, examining the similarities and differences of the overall distribution of the measured features and time-based changes throughout the interactions. 
The features \acl{f0}, intensity, and \acl{ar} were selected for analysis.
Results show significant differences in a majority of the cases for two of the three selected features as well as insights regarding the participants' speech behavior during the interaction.
These outcomes provide a look into further aspects of \ac{hds} and \ac{dds} and speech-related features in conversation analysis, which may help studies in topics like addressee detection or \ac{hci}.

\section{Introduction}
\label{sec:introduction}

Nowadays, we are witnessing in our everyday lives an ever-growing presence of devices with spoken interaction capabilities, like \acp{pa}, speech-activated cars, hands-free medical assistants, and \acp{its}, to name a few.
The question arises, therefore, whether different speech patterns and characteristics emerge in such \ac{hci} than in \ac{hhi}; and if yes, which.

It has been demonstrated that humans may change their speech behavior when interacting with computer-based systems.
One way of measuring such changes is in terms of linguistic similarity between the interlocutors.
In various \ac{hci} experiments, participants have been shown to speak differently to computers in general, and also change their speech during the interaction (see \citet{Branigan2010linguistic}, for examples).
However, these are \acp{hci}  that emphasize the comparison between different configurations of the system itself \citep[e.g.,][]{Levitan2016implementing}.
Moreover, no direct comparison between human-directed and computer-directed speech was performed.

In this paper, we present conversation-level analyses of speech changes in \acp{hhci} with an Amazon Alexa device (Echo Dot, 2nd generation) as the computer-based interlocutor.
We chose to analyze three phonetic features: \ac{f0}, intensity, and \ac{ar}.
These analyses show the differences in the human interlocutor's behavior when addressing the confederate human speaker or the computer interlocutor.
Examples of distributional and temporal changes across the interaction are given.
These analyses are performed on the \acf{vacc}~\citep{Siegert2018VACC}, which comprises two experimental scenarios: the Calendar Module (formal interaction) and the Quiz Module (informal interaction).

Other studies, like \citet{Shriberg2013addressee} and \citet{vanTurnhout2005identifying}, used similar corpora to study automatic addressee detection.
The present work does not set detection and classification as its goal, but rather aims to provide insights and measures that might be useful for such tasks.
The results of the analyses show that the differences between the distributions for the features \ac{f0} and intensity in \ac{hds} and \ac{dds} contexts were significant in \SI{74}{\percent} and \SI{89}{\percent} of the cases, respectively, while for \ac{ar} in only \SI{13}{\percent}.
This sheds light on the similarities and differences in speech when addressing humans and computers.

\section{Dataset}
\label{sec:dataset}

The \ac{vacc} \citep{Siegert2018VACC} was utilized to examine differences in \ac{hds} and \ac{dds}.
This corpus consists of conversations between one or two human interlocutors with the 2nd generation of the commercial smart speaker Amazon Echo Dot, which uses the skills and voice of the virtual assistant Alexa.
The conversations comprise a formal and an informal scenario conducted either with the participant alone or together with a confederate accompanying person, which allows investigating how humans address computer-based systems.

% a formal and an informal one, are designed. The scenarios are conducted by the participants alone and with an accompanying person. Furthermore, questionnaires are used to get a self-evaluation of the participants in terms of their experience of the interaction and their conscious changes in voice  and behaviour while addressing a technical system. Additionally, also their experience with technical systems and the evaluation of the utilized commercial voice assistant is retrieved via questionnaires. The corpus consists of high-quality microphone recordings of 27 German speaking subjects, all students at the University Magdeburg.

%\begin{table}[t]
%	\caption{Summary of the \acs{vacc} metadata}
%	\label{tab:dataset_metadata}
%	\begin{center}
%		\begin{tabular}{l@{\hspace{2cm}}l}
%			\toprule
%			Language 					& German												  \\
%			\#Participants			 	& 27 (14 female / 13 male)								  \\
%			Age mean	 				& Mean 24 $\pm$ 3.32 years								  \\ %Min: 20; Max: 32
%			\#Interactions				& 108												   	  \\
%			Total time			 		& \SI{17}{\hour} \SI{7}{\minute} (mean: \SI{31}{\minute}) \\
%			Total number of utterances	& $~\sim$13,500											  \\
%			\bottomrule
%		\end{tabular}
%	\end{center}
%\end{table}

\subsection{Setting and participants}
\label{subsec:setting_and_participants}

\ac{vacc} contains recordings of 27 German native speaking students from Otto von Guericke University Magdeburg.
Each speaker participated in four recordings, for a total of 108 interactions (27 participants $\times$ 2 scenarios $\times$ 2 conditions).
The total recording time is \SI{17}{\hour} \SI{7}{\minute} (\SI{31}{\minute} on average per interaction) containing \num{\sim 13500} utterances.
The number of female (14) and male (13) participants is nearly equal, and their ages range from 20 to 32 years (mean~24.11; sd~3.32).
The participants came from different study programs, including computer science, engineering, humanities, and medical sciences.
Thus, this dataset is not biased towards students with stronger technical background.

An experiment with a participant consists of four interactions:
A formal and an informal scenario, each carried out in solo and confederate conditions.
An interaction was finished either by reaching its aim or by stopping it to avoid participant frustration in case no further progress could be made.

In the first scenario, the Calendar Module, each participant was asked to make appointments with a project partner.
The participant's calendar was stored online and was only accessible via Alexa's voice commands.
In the solo condition, the participants only received written information about the confederate's available dates and had to interact with Alexa on their own.
In the confederate condition, the confederate provided the relevant information.
Therefore, the participant had to interact with both Alexa and the confederate to find available time slots for the appointments.

In the second scenario, the Quiz Module, the participant answered trivia questions like \enquote{When was Albert Einstein born?}, which are mostly assumed to be too difficult to answer correctly without Alexa's help.
In the solo condition, the participants had to find the correct strategy for formulating questions for Alexa on their own own, whereas in the confederate condition the participant and the confederate teamed up to decide on a strategy. 

\subsection{Annotations}
\label{subsec:annotations}

Each utterance in an interaction was annotated with its speaker, context, and textual transcription.
The speaker of each utterance could be the participant, Alexa, or the confederate.
The context marked the type of interaction of the utterance, which include \ac{hds}, \ac{dds}, cross-talk, off-talk, laughter, and more.
To deal with clearer data, only \ac{hds} and \ac{dds} contexts were used for analysis in this paper (see \cref{sec:method}).
The transcription was obtained using the Google Cloud Speech API automatic speech recognition service.

\section{Method}
\label{sec:method}

To make the comparison between \ac{hds} and \ac{dds} more direct, we selected only those interactions where the participants talked with both the device and the confederate.
The remaining \SI{50}{\percent} of the dataset includes one calendar task and one quiz task for each of the 27 participants.
This subset was analyzed based on the audio signals of the interactions only.
The turn annotations were used to determine to which of the three speakers the measured values should be ascribed.
The text transcription was not utilized for this study.

Each interaction was analyzed using the audio from the participant's microphone (where both the confederate and the device are audible as well) and the annotations described in \cref{subsec:annotations}.
To increase temporal resolution, the audio signals were cut into two-seconds slices.
A single slice always contains audio from a turn of a single speaker, any remainder shorter than \si{2} seconds gets a slice of its own.
For example, a turn of \SI{5.2}{\second} in length was sliced into three slices of \SI{2}{\second}, \SI{2}{\second}, and \SI{1.2}{\second}.
Each of the features was then analyzed within a single slice.

The following phonetic features were analyzed:\\
\textbf{\Acf{f0}} -- the mean pitch measured within the audio slice with automatic time step selection and a range between \SIlist{60;350}{\hertz}.\\
\textbf{Intensity} -- the mean intensity measured within the audio slice with automatic time step selection.\\
\textbf{\Acf{ar}} -- the ratio of number of syllables to phonation time within the audio slice, as described in \citet{DeJong2009arcitulcationrate}.\\
All features were measured using Praat\footnote{version 6.0.35} \citep{Boersma2001praat} scripts that processed the signal of the participant's microphone.
To filter out noise and concentrate on the more characteristic speech style, only values between the first and third quartiles were taken into account for the non time-based analyses.
Furthermore, turns not annotated as \ac{hds} or \ac{dds} (e.g., cross-talk or off-talk) were also ignored.

\section{Results}
\label{sec:results}

As a first measure of speech characteristics, the means, medians, and \aclp{sd} of the selected features in the participants' speech in each of the interactions were calculated in \ac{hds} and \ac{dds}.
The absolute values reflected in the means and medians can shed light on the overall range of values used with each of the two interlocutors,
for example, due to different gender (Alexa was always set with a female voice and the confederate was always male) or assumed comprehension capabilities of humans and computers.
The \aclp{sd} show the variability of a feature with each interlocutor, which may indicate a different production style.

Each target feature was measured and listed chronologically throughout the interaction.
These lists were divided into four distributions based on speaker and context: participant talking to the confederate, participant talking to Alexa, confederate talking to participant, and Alexa talking to participant.
The contrast between \ac{hds} and \ac{dds} is observable within the participant's speech only, which was active in both contexts.
The significance level of the difference between these two distributions was measured using a two sample t-test with $\alpha=0.05$.

\cref{fig:hds_dds_dist_pitch_comparison} shows examples of the distributions of the participant's \ac{f0} in \ac{hds} and \ac{dds} contexts.
Since the device always used the default Alexa female voice and the confederate was always male, there is a natural gap between their \ac{f0}.
This gap leaves room for convergence to occur, i.e., when two interlocutors become more similar to each other over time with respect to some measured feature.
In \SI{74}{\percent} of the cases out of the 54 analyzed interactions the difference of means of the participant's \ac{hds} and \ac{dds} \ac{f0} distributions was significant.
In \SI{85}{\percent} of the cases where the difference was significant, more of the probability mass of \ac{dds}'s distribution contained higher values than \ac{hds}'s, and therefore more similar to Alexa.

\begin{figure}[t]
	\centering
	\subfigure
	[An example of \ac{hds} and \ac{dds} distribution densities with a \emph{significant} difference (p-value$\ll$0.0001, $\alpha=0.05$) extracted from the \ac{f0} measures of participant 20171127A in the Quiz task.]
	{\includegraphics[width=0.45\textwidth]{20171127A_Quiz_01_pitch_dist}
		\label{fig:hds_dds_dist_signif}} 
	\hfill % no empty line here to avoid staring a new paragraph (figures will be vertically aligned)
	\subfigure
	[An example of \ac{hds} and \ac{dds} distribution densities with a \emph{non-significant} difference (p-value=0.71, $\alpha=0.05$) extracted from the \ac{f0} measures of participant 20171127C in the Calendar task.]
	{\includegraphics[width=0.45\textwidth]{20171127C_Calendar_02_pitch_dist}
		\label{fig:hds_dds_dist_nonsignif}}
	\caption
	[Distribution densities of f$_0$ in \ac{hds} and \ac{dds}]
	{A \emph{significant} \subref{fig:hds_dds_dist_signif} and a \emph{non-significant} \subref{fig:hds_dds_dist_nonsignif} difference between distribution densities of participants' \ac{hds} and \ac{dds} \ac{f0}.
		The colors represent distributions of Alexa (red), the confederate (green), and the participant (blue).
		The line style differentiates between \ac{hds} context (dashed line) and \ac{dds} context (solid line).}
	\label{fig:hds_dds_dist_pitch_comparison}
\end{figure}

\cref{fig:hds_dds_violin_intensity_comparison} shows examples of the distributions of the participant's intensity in \ac{hds} and \ac{dds} contexts.
Unlike in the case of \ac{f0}, absolute measured values may not be as meaningful due to the device's and the confederate's location relative to the participant's microphone.
As explained in \cref{sec:method}, the signal from the participant's microphone was used for the difference analyses.
This means that the absolute values of the participant's intensity  in \ac{hds} and \ac{dds} can be compared directly, but only indirectly with Alexa's and the confederate's.
Therefore, in \cref{fig:hds_dds_violin_intensity_comparison} the differences in distribution and frequency can be compared within a context, but the values should only be compared within the participant's speech (in blue).
In \SI{89}{\percent} of the cases out of the 54 analyzed interactions the difference of means of the participant's \ac{hds} and \ac{dds} intensity distributions was significant.
Moreover, participants tended to speak to Alexa with a louder voice than to the confederate.

\begin{figure}[t]
	\centering
	\subfigure
	[An example of \ac{hds} and \ac{dds} values with a \emph{significant} difference (p-value$\ll$0.0001, $\alpha=0.05$) extracted from the intensity measures of participant 20171127A in the Quiz task.]
	{\includegraphics[width=0.47\textwidth]{20171127A_Quiz_01_intensity_violin}
		\label{fig:hds_dds_violin_signif}} 
	\hfill % no empty line here to avoid staring a new paragraph (figures will be vertically aligned)
	\subfigure
	[An example of \ac{hds} and \ac{dds} values with a \emph{significant} difference (p-value=0.55, $\alpha=0.05$) extracted from the intensity measures of participant 20171127C in the Calendar task.]
	{\includegraphics[width=0.47\textwidth]{20171127C_Calendar_02_intensity_violin}
		\label{fig:hds_dds_violin_nonsignif}}
	\caption
	[Distribution densities of intensity in \ac{hds} and \ac{dds}]
	{A significant \subref{fig:hds_dds_violin_signif} and non-significant \subref{fig:hds_dds_violin_nonsignif} difference between extracted values of participants' \ac{hds} and \ac{dds} intensity.
		The colors represent distributions of Alexa (red), the confederate (green), and the participant (blue).
		\ac{hds} is plotted in the right, and \ac{dds} in the left of the plot.
		The width of the box represents the frequency of the values and the \enquote*{+} sign marks their respective means.}
	\label{fig:hds_dds_violin_intensity_comparison}
\end{figure}

The differences of \acf{ar} distributions in \ac{hds} and \ac{dds} were calculated as well.
In \SI{13}{\percent} of the cases out of the 54 analyzed interactions the difference of means of the participant's \ac{hds} and \ac{dds} \ac{ar} distributions was significant.
This shows that the participants largely spoke at the same speed with the confederate and the device.
It was found that the articulation rate was lower in some specific cases where the participant tried to improve her/his intelligibility to the system, specifically when the system's output indicated that it could not understand the participant's utterance.
While such utterance-level changes are interesting and may point to a temporary change in behavior, a more detailed analysis is outside the scope of this study, which concentrates on interaction-level behavior.

Looking at the distribution differences of the selected features in \ac{hds} and \ac{dds} sheds light on the general speech behavior in these contexts.
However, this analysis leaves out an important aspect of spoken interaction, namely the time dimension.
While the static measures of distributions show the overall range and frequency of the values, temporal analysis adds the information as to how they changed over time.
Adding the time dimension gives an overview of the interaction's structure and reveals fine-grained information regarding its characteristics, such as turn lengths, turn switching, pauses, density of a speaker's utterances, convergence or divergence effects, etc.
For example, \cref{fig:hds_dds_time_pitch} shows a case where the the absolute \ac{f0} values are roughly the same in \ac{hds} and \ac{dds}, namely around \SI{150}{\hertz}, but the behavior of the participant is different.
In the \ac{dds} context, the participant generally keeps a stable distance from Alexa's voice, whereas in the \ac{hds} context the \ac{f0} values are closer to the confederate.
In both contexts, the participant's \ac{f0} starts around \SI{150}{\hertz}.
However, in \ac{hds} the minimum \ac{f0} is only slightly below this initial value, whereas in \ac{dds} it drops as far as \SI{25}{\hertz} lower.
An example for the intensity feature is shown in \cref{fig:hds_dds_time_intensity}.
Unlike the previous example, here the absolute values steadily differ by about \SI{5}{\decibel}, but the overall change is similar.
That is, in both cases the intensity rises from the beginning to around a quarter of the interaction's duration, and then decreases again until the end (in \ac{hds} more quickly than in \ac{dds}) down to approximately the same value as at the beginning.

Since \cref{fig:hds_dds_time_comparisons} shows two examples of the Quiz task performed by two different participants, it is possible to compare the structure of these interactions as well.
As described in \cref{sec:dataset}, the Quiz task in the confederate condition is designed so that the two human speakers needs to find an effective way to solve the questions using Alexa.
After improving their strategy, the lead should ultimately be taken by the participant, who interacts with Alexa to solve the questions as quickly and correctly as possible.
In both examples, the interaction starts with relatively short turns and rapid context changes.
This might be ascribed to the fact that the participant is still trying to figure out the best way to interact with Alexa and the confederate.
Then, sometime after the middle of the interaction, there is a larger block of \ac{dds} only, followed by some more turns of \ac{hds}.
Finally, the interactions end with another, shorter, block of \ac{dds}, in which the participants finish the last questions of the quiz.
This structure was found to be typical for the Quiz task.

\begin{figure}[t]
	\centering
	\subfigure
	%	[An example of \ac{f0} changes over the time of the interaction.
	%	In \ac{dds}, the speaker starts a bit above \SI{150}{\hertz} and ends slighly beneath it, generally maintaining the distance from the device's values.
	%	In \ac{hds}, the contour starts around the same value as in \ac{dds}, but gradually goes down towards the confederate and then up again, staying in the same range of values as the human speaker.
	%	This shows a different behavior with similar values.]
	{\includegraphics[width=0.49\textwidth]{20171129A_Quiz_01_pitch_time}
		\label{fig:hds_dds_time_pitch}} 
	\hfill % no empty line here to avoid staring a new paragraph (figures will be vertically aligned)
	%	\subfigure
	%	[An example of intensity changes over the time of the interaction.
	%	In both \ac{hds} and \ac{dds} contexts, there are a rise and a fall around the same time in the participant's values.
	%	However, the absolute values differ.
	%	This shows a similar behavior with different values.]
	{\includegraphics[width=0.49\textwidth]{20171129B_Quiz_02_intensity_time}
		\label{fig:hds_dds_time_intensity}}
	\caption
	[Temporal f$_0$ trends comparison of \acs{hhi} and \acs{hci} turns]
	{The changes in pitch (left) and intensity (right) over time in \ac{dds} (upper part) and \ac{hds} (lower part).
		The time spans on the x-axis are represented by turn slices, as explained in \cref{sec:method}, and the y-axis shows the value of the feature.
		A slice's background color indicates the speaker in this slice and the circle with the same color, the measured value of the feature in it.
		Alexa's voice is shown in red, the confederate in green, and the participant in blue.
		The lines are smoothed values calculated by LOESS \citep{Cleveland1988locally}.}
	\label{fig:hds_dds_time_comparisons}
\end{figure}

\cref{tab:results_summary} summarizes the results.

\section{Discussion}
\label{sec:discussion}

\cref{sec:results} presented the results for the features \ac{f0}, intensity, and \acf{ar}.
The first two show a greater degree of difference between \ac{hds} and \ac{dds}, and the latter, a smaller one.
One possible explanation for the different \ac{f0} distributions is the natural difference in male and female \ac{f0} (Alexa used a female voice while the confederate was always male) and the fact that humans sometimes tend to match their \ac{f0} to the interlocutor.
In that case, the results shown here point to the fact that the participants generally treated Alexa as a human interlocutor with regards to \ac{f0} behavior, as opposed to, for example, matching only the human interlocutor and talking to the computer-based interlocutor using the same \ac{f0}.
A similar effect was found for intensity.
Since the device and the confederate were spaced approximately at the same distance from the participants, there was no apparent reason for the participants to speak more loudly with either interlocutor.
Therefore, an explanation of the tendency to speak more loudly to the device may come from the intuition that a computer-based system has a harder time to understand human speech and therefore needs a clearer signal.
Another explanation may be the illusion that Alexa feels more distant than the human interlocutor, because Alexa is not an embodied agent.
Keeping in mind that an interaction aims to be as efficient as possible using minimum amount of energy, it seems like changing these features helped -- or at least felt like helping -- the participant to interact more efficiently with the device.
This is not the case with \ac{ar}, which shows a lower degree of differences.
Slower, more carefully articulated speech, occurs less often in regular speech than louder speech or higher pitch.
Such enhanced articulation not only takes longer to produce, but also requires more effort, making it a less preferred way to communicate, unless it is necessary.
In a somewhat formal, experimental setting, participants are likely to speak more slowly than usual, and the motivation to complete the task in a short time does not encourage them to speak even more slowly.
This supports the hypothesis that extra slow speech would only be used when necessary, e.g., when a repetition is required due to a misunderstanding of an utterance.
Even in that case, the overall \ac{ar} tends to increase afterwards, to make the interaction more fluent again.

Future work may go in two main directions, both concerning a temporal aspect of interaction.
The first has do to with analysis on the speech signal level, where the changes in measures over time can capture phenomena like convergence or divergence.
In a more comprehensive analysis in this direction, more detailed patterns may emerge.
Such an analysis can concentrate on one context or on comparing patterns in both \ac{hds} and \ac{dds}.
Additionally, more features can be measured to reveal more details regarding speech behavior.
The second potential direction may highlight behavioral patterns of the conversation and turn levels.
This can include a closer examination of the interaction structure as a whole, the dynamics of turn changes, pauses and repetitions, etc.
Such an analysis can be performed on interactions in solo and confederate condition to inspect whether humans deal with the same task differently with a computer alone, than when another human is involved.

\begin{table}[t]
	\centering
	\caption
		[Percentage of significantly different interactions]
		{Summary of results.
		The percentage of interactions in which the difference of distribution means was significant for each feature, and their mean and \acf{sd}.}
	\label{tab:results_summary}
	\begin{tabular}{lSSS}
		\toprule
		& \acs{f0} 						& {intensity}				& \acs{ar}									\\
		signif. diff.					& \SI{74}{\percent}			& \SI{89}{\percent}		& \SI{13}{\percent} \\
		\acs{hds} mean (\acs{sd}) 		& 10.5\,\si{\hertz}			& 2.95\,\si{\decibel}	& 0.627				\\
		\acs{dds} mean (\acs{sd}) 		& 10   \,\si{\hertz}		& 2.61\,\si{\decibel}	& 0.634				\\
		\bottomrule	
	\end{tabular}
\end{table}

\section{Conclusion}
\label{sec:conclusion}

In this paper, we presented an analysis of phonetic features in a study based on a subset of a human-human-computer corpus, which includes two tasks with 108 interactions in total.
Three interlocutors participate in the interactions: the participant, a confederate, and an Amazon Alexa device.
The features \ac{f0}, intensity, and \ac{ar} were analyzed for each of the three interlocutors across each interaction.
Based on the dataset's turn annotations, the utterances of the speakers were categorized as either \ac{hds} or \ac{dds} context.
First, the participant's speech in both contexts was examined by comparing the distributions of the measured values in each context.
Then, the difference of the distributions was checked for significance.
Finally, the temporal changes of the features across the interaction were examined as well.

The difference between the distributions was significant in \SI{74}{\percent} and \SI{89}{\percent} of the interactions for \ac{f0} and intensity, respectively, and in \SI{13}{\percent} for \ac{ar}.
As for the temporal analysis, different patterns of changes were observed, like cases where the participant accommodated to the human interlocutor but not the computer and cases where a similar behavior was observed in both contexts.

% *** Alexa Interspeech paper ***

\section{abstract}

This study examines how the presence of other speakers affects the interaction with a spoken dialogue system.
We analyze participants' speech regarding several phonetic features, viz., fundamental frequency, intensity, and articulation rate, in two conditions:
with and without additional speech input from a human confederate as a third interlocutor.
The comparison was made via tasks performed by participants using a commercial voice assistant under both conditions in alternation.
We compare the distributions of the features across the two conditions to investigate whether speakers behave differently when a confederate is involved.
Temporal analysis exposes continuous changes in the feature productions.
In particular, we measured overall accommodation between the participants and the system throughout the interactions.
Results show significant differences in a majority of cases for two of the three features, which are more pronounced in cases where the user first interacted with the device alone.
We also analyze factors such as the task performed, participant gender, and task order, providing additional insight into the participants' behavior.

\section{Introduction}
\label{sec:introduction}

In recent years, the market for commercial \acp{va} has rapidly grown.
For example, Microsoft Cortana had 133 million active users in 2016 \citep{Osborne2016why} and Echo Dot was Amazon's best-selling product between 2016 and 2018 \citep{Dickey2017echo}.
Furthermore, \SI{72}{\percent} of people who own a smart speaker say they often use their devices as part of their daily routine \citep{Kleinberg2018ways}.

The big advantage of \acp{va} is their simple operation.
Using nothing but speech commands, users can perform tasks like playing music, searching the web, shopping online, etc.
In the future, we are likely to witness an ever-growing presence of devices with spoken interaction capabilities, like speech-activated cars, hands-free medical assistants, and intelligent tutoring systems.
This will increase the demands on voice-activated devices even more, as they will need to support more functionalities in a way that is comfortable and intuitive for the users.
Additionally, it can be expected that such devices will be used not only by individuals, but also in more social contexts, i.e., where multiple humans are involved.
Therefore, we find it important to investigate not only \aclp{hci}, but also \acp{hhci}.

Besides making the operation of such voice-activated systems simple and user-friendly, \acp{va} also aim to let users interact with them in a familiar, natural manner.
One property of natural interactions is the tendency to accommodate to the specific situation and interlocutors to  make the interactions more fluent and efficient \citep{Giles1991CAT,Gallois2015CAT}.
Linguistic accommodation is one aspect of this phenomenon, and it is found in various \ac{hhi} experiments \citep[e.g.,][]{Pardo2017phonetic,Schweitzer2017social}.
In various \ac{hci} experiments, it has been shown that participants speak differently to computers in general, and also change their speech behavior during the interaction, e.g., by \citet{Branigan2010linguistic} and \citet{Levitan2016implementing}.
However, these interactions include only the computer-based interlocutor and emphasize the comparison between different configurations of the system itself.
%Furthermore, these studies have never used an actual commercial \ac{va} system.
Moreover, they only examine the influence of the system's speech output on the user, but not the influence of other interlocutors.

The question tackled in this paper is whether and to what extent speaking to a second human interlocutor in addition to Alexa influences the accommodation in interaction with a \ac{va}.
More generally, we investigate whether users speak differently towards a computer-based system when another human participates in the interactions.
%To investigate this question and other factors, we utilize the \ac{vacc} \citep{Siegert2018VACC}, which comprises human-computer interactions of two different tasks, each of them performed once with the \ac{va} alone and once with the \ac{va} and a human confederate.
This was done using a set of interactions of participants with a \ac{va} alone or with a \ac{va} and a confederate.
Thus, it allows us to analyze the participants' accommodation to the \ac{va} in both social conditions.

% Empirical evidence of entrainment in human-human conversations has been documented for acoustic-prosodic features suchas intensity [7, 8, 9], speaking rate [2], and pitch [8, 9].
% Humans have been shown to entrain to their interlocutorâ€™s languageat the lexical [10] or syntactic [11, 12] level, and on linguisticstyle [13, 14, 15].
% Motivated by Communication Accommoda-tion Theory (CAT) [16], which holds that speakers converge toor diverge from their interlocutors in order to attenuate or accen-tuate social differences, many studies have found links betweenentrainment and positive social behavior: entraining conversa-tional partners are perceived as more socially attractive, morecompetent, more likable, interactions with them as more suc-cessful, entrainment is positively correlated with learning gainsin automatic tutoring system and task success in Map Tasks[17, 18, 5, 3, 19, 1, 20, 21]

\section{Dataset}
\label{sec:dataset}

To analyze the influence of a confederate speaker in \ac{hci}, we used the \acf{vacc} \citep{Siegert2018VACC}.
This corpus comprises balanced human-computer (solo condition) and human-human-computer (confederate condition) interactions with a 2nd generation Amazon Echo Dot that uses the skills and female voice of the virtual assistant Alexa.
The first human speaker is the participant, which always takes part in the interactions.
The interactions consist of Calendar and Quiz tasks, where the former simulates a formal situation and the latter a rather informal situation.
This corpus, which alternately introduces a confederate speaker into otherwise similar conversations, allows investigating the influence of the confederate on the behavior of the participant.

\ac{vacc} contains recordings of 27 (14 female) German native speakers with a mean age of 24 years (sd 3.3).
Each participant performed the Quiz and Calendar tasks in both solo and confederate conditions, for a total of 108 interactions.
An interaction was finished either by completing the task or by stopping it prematurely in case no further progress could be made, to avoid participant frustration.
The latter, however, happened only a few times.
Approximately \num{13500} utterances were recorded, stretching over total recording time of \SI{17}{\hour} \SI{7}{\minute} (\SI{31}{\minute} on average per interaction).
The permutations of the tasks, conditions, and their order were balanced.

In the Calendar task, the participant made several appointments in pre-defined weeks with the confederate.
The participant's calendar was stored online, accessible only via Alexa.
In the solo condition, the participants got written information about the confederate's availability, whereas in the confederate condition, the confederate could be asked directly about it, resulting in a \emph{\ac{hhi}}.
In the Quiz task, the participant answered trivia questions like, \enquote{When was Albert Einstein born?}
Although Alexa was not always able to immediately provide a full answer to all the questions, information could be incrementally gathered in multiple steps.
Here, the participant solved the quiz alone in the solo condition, or teamed up with the confederate so the two could discuss the question asking strategy.
The Quiz task was generally less formal than the Calendar task.
The confederate interacted only with the participant whenever he was involved.

The three interlocutors were arranged at an approximately equal distance from each other.
The two human speakers were seated and the Echo device was placed on a table.
More information about the recording setup and equipment is described by \citet{Siegert2018VACC}.
Turn times and speakers were manually annotated.

% \begin{figure}[!h]
% \begin{center}
% %\fbox{\parbox{6cm}{
% %This is a figure with a caption.}}
% %\includegraphics[scale=0.5]{image1.eps} 
% \includegraphics[width=0.75\linewidth]{figures/IMG_4798.jpg} 
% \caption{A snapshot of the data collection setup.The confederate speaker (left side) and the participant (right side) are sitting around a table, where the voice assistant (Amazon Alexa Echo Dot) is located.}
% \label{fig:wohnzimmer}
% \end{center}
% \end{figure}
% 
%	The recordings were conducted in a living room-like surrounding, see \cref{fig:wohnzimmer}.

% The aim of this setting was to enable the participant to get into a natural communication atmosphere (in contrast to the distraction of laboratory surroundings).
% The participant sat on the sofa (right side of the photo in Fig.~\ref{fig:wohnzimmer}) and interacted with the voice assistant system, placed on the table in the middle.
% The confederate speaker (present only in the two-person variants of each scenario) sat on the armchair (left side of the photo in Fig.~\ref{fig:wohnzimmer}).
% The positions were identical for all recordings of all participants to ensure comparability.

%Two high-quality neckband microphones (Sennheiser HSP 2-EW-3) were used to capture the voices of the participant and the confederate speaker.
%\todo{which one is used for the analyses?}
%Additionally a high-quality shotgun microphone (Sennheiser ME 66) captured the overall acoustics and the output of Amazon's Alexa.
%The recordings were stored in WAV format with \SI{44.1}{\kilo\hertz} sample rate and 16 bit resolution.
%The recordings were manually separated into utterances, which were additionally annotated with its speaker, context, and textual transcription.
%The speaker of each utterance could be the participant, Alexa, or the confederate.
%The context marked the type of interaction of the utterance, which include \ac{hds}, \ac{dds}, cross-talk, off-talk, laughter, and more.
%To deal with clearer data, only \ac{hds} and \ac{dds} contexts were used for analysis in this paper.
%The transcription was obtained using the Google Cloud Speech API automatic speech recognition service.
%\cref{tab:dataset_charact} summarizes the dataset characteristics.

%\begin{table}[t]
%	% \captionsetup{format=plain,justification=raggedleft,width=.4\textwidth,hangindent=0pt,skip=500pt}
%	\centering
%	\caption{\ac{vacc} dataset characteristics}
%	\label{tab:dataset_charact}
%	\begin{tabular}{L{3cm}L{3cm}}
%		\toprule
%	     Participants 				& 27														\\
%	     Sex 						& Male 13 / Female 14 										\\
%	     Total Recorded Data 		& \SI{17}{\hour} \SI{7}{\minute}							\\
%	     Experiment Duration 		& Mean: 31 min 												\\
%		  Age (years)				& Mean 24 (Std: 3.32)										\\	% Min: 20; Max: 32  \\
%	     Language 					& German													\\
%%	     Annotation 				& Transcription, Addressee, Laughter, Cross-Talk, Off-Talk	\\ 
%%	     Supplementary self-reports & Evaluation of interaction, AttrakDiff, Speaking style, Experiences in interacting with voice assistants\\
%		\bottomrule
%	\end{tabular}
%\end{table}

\section{Method}
\label{sec:method}

\begin{table}[t]
	\centering
	\caption
		[Percentage of significantly different interaction pairs]
		{Percentage of interaction pairs with significant differences with respect to each target feature with all the interactions together and separated by order tasks.}
	\label{tab:signif_conditions}
	\sisetup{table-format=3.0}
	\begin{tabularx}{\linewidth}{XSSS}
		\toprule
		\thead[l]{feature} & {\thead{any order}} & {\thead{solo first}}	& {\thead{confederate first}}	\\
		\midrule
		\acs{f0}	& 67	& 72	& 60 \\
		intensity 	& 67	& 76	& 56 \\
		\acs{ar}	& 30	& 31	& 28 \\
		\bottomrule	
	\end{tabularx}
\end{table}

All interactions from the \ac{vacc} were used for the analyses.
The comparisons were performed on pairs of interactions of the same task in the two conditions presented above.
%This puts the focus on the difference between those interactions in which the participant's speech may have been influenced by the confederate's speech and those where it could not be.
Only the audio signals of the interactions were used for analysis, as recorded by the headset microphone of the participant, which captured the speech of the confederate and Alexa as well.
Since the participants sat at an equal distance from Alexa and the confederate, this eliminates any spatial influence on their speech, e.g., in terms of intensity.
The turn annotations were used to determine to which of the three speakers the measured values belong.

To increase temporal resolution, the audio signals were split into two-second slices.
This duration is short enough to provide a decent temporal resolution shorter than a turn, but still long enough to calculate time-dependent features like \acf{ar}.
Splitting the turn also creates equal, consecutive, and more comparable time units for an interaction without introducing artificial boundaries by dividing it into a pre-defined number of parts \citep{Silber-Varod2018prosodic}.
This is especially important for the temporal analysis (\cref{subsec:temporal_analysis}).
Preliminary experiments with the corpus showed only very small changes in feature measurements with slices of longer duration.
The slicing was done per turn, so that a single slice contains the audio of a single speaker.
Any remainder of a turn duration got a slice of its own.

The following phonetic features were targeted, as they are in the focus of convergence research \citep{Levitan2011measuring,Natale1975Conv,Gregory1993Voice}:
\begin{description}[wide=0pt, leftmargin=0.5\parindent, nosep]
	\item[\Acf{f0}] mean pitch measured within the audio slice with automatic time step selection and a range between \SIlist{60;350}{\hertz}.
	\item[Intensity] mean intensity measured within the audio slice with automatic time step selection.
	\item[\Acf{ar}] ratio of number of syllables to phonation time within the audio slice, as described by \citet{DeJong2009arcitulcationrate}.
\end{description}

All features were measured in each slice individually using Praat\footnote{version 6.0.35} \citep{Boersma2001praat} scripts.
To filter out noise and concentrate on the more characteristic speech style, only values from the second and third quartiles were taken into account.
Furthermore, turns not annotated as speech for either of the speakers (e.g., cross-talk or off-talk) were also ignored.

\section{Results}
\label{sec:results}

\begin{figure}[t]
	\centering
	\includegraphics[width=\linewidth]{barplot_signif_cases}
	\caption
		[short caption]
		{Percentage of instances with a significant difference between solo and confederate conditions in each case.
		A case is a combination of the factors \emph{task}, \emph{feature}, and \emph{order}.
		For example, the case \emph{Q.intensity.2} contains the comparisons of intensity in interactions of the Quiz task where solo condition was performed second (and the confederate condition first).}
	\label{fig:signif_cases_ordered}
\end{figure}

Two separate analyses were carried out: distributional and temporal.
The first looks at global differences on the interaction level of the participant's and the computer-based interlocutor's productions between solo and confederate conditions.
It also checks whether the order in which the tasks were performed had any influence on the changes as well (see \cref{tab:signif_conditions,fig:signif_cases_ordered}).
The second examines time-based, continuous changes in the proximity between the participant's and the device's productions with emphasis on the \emph{condition} factor, and then also provides additional insights for the factors \emph{sex}, \emph{task}, and \emph{order} (\cref{fig:condition_convergence_comparison,fig:alluvial}).

\subsection{Distributional analysis}
\label{subsec:distributional_analysis}

\begin{figure*}[t]
	\centering
	\includegraphics[width=\linewidth]{20171127B_Quiz_pitch}
	\caption
		[short caption]
		{A comparison between the behavior of the \ac{f0} feature in solo condition (left) and confederate condition (right).
		The lines represent the \ac{loess} smoothing trend lines of the participant (blue) and Alexa (red).
		Omitted turns, e.g., turns of the confederate and turned removed as explained in \cref{sec:method} are not colored (gray).
		The vertical bars in the upper half represent the turns of the participant (blue) and Alexa (red).
		The color-scaled vertical bars at the bottom half are the convergence/divergence level of the participant over time as calculated in \cref{eq:accommodation}.
		Blue areas represent convergence while red areas represent divergence.
		The darker the color, the greater the effect, with white color pointing to points of no change (or synchrony, in segments with both trends moving the same way).}
	\label{fig:condition_convergence_comparison}
\end{figure*}


The distributional analysis examines the differences between the participant and Alexa's speech in solo and confederate conditions in terms of the general behavior of the participants with respect to the target features.
This general behavior is determined by the set of values of the target features produced by the participants in each condition.
Since this analysis checks whether the participants behave differently as a whole towards the non-human speaker, the temporal order of the values is not considered (cf.\ \cref{subsec:temporal_analysis}).

To detect these differences, the distribution of their respective values in the solo and confederate conditions in each interactions pair were compared.
This was done by using the two-sample Wilcoxon test \citep{Wilcoxon1945individual}, with $\alpha = 0.05$ with the null hypothesis that similar value distributions of the target feature were used in both conditions.
A significant result of the test means that the participant produced the respective feature differently when interacting with Alexa alone compared to when the confederate participated as well.
\cref{tab:signif_conditions} shows the percentage of interaction pairs, in which the null hypothesis was rejected, i.e., that the feature was utilized differently by the participant in each condition.
Since chronologically, one of the conditions needed to precede the other, the percentages were also calculated separately for the cases where tasks were performed first in the solo condition (and then in the confederate condition), and vice versa.
This separation shows whether interacting first with Alexa alone, without any human input, influenced the vocal behavior of the participants.
As there were no breaks between the tasks, the only factors for change were the order of the conditions and the involvement of another human speaker.
Indeed, the percentages of significant differences when interacting first only with Alexa were higher by \SI{12}{\percent}, \SI{20}{\percent}, and \SI{3}{\percent} for \ac{f0}, intensity, and \ac{ar}, respectively.

\cref{fig:signif_cases_ordered} further breaks down the differences between interaction pairs and introduces the factor of the performed task.
In line with the tendency shown in \cref{tab:signif_conditions}, the features \ac{f0} and intensity have the highest percentages of significant cases regardless of the performed task, and the tasks performed first show higher percentages of different distributions.
In the lower percentages, it is the task, rather than the target feature, that shows differences between the cases.
And last, for \ac{ar}, with the lowest percentages, there is a clear difference between the Quiz and the Calendar tasks.
All in all, the \emph{task} factor was a good indicator only for the feature with the lowest difference percentage and the \emph{order} factor was more informative for the features with higher percentages.

\subsection{Temporal analysis}
\label{subsec:temporal_analysis}

Another way to look at accommodation in an interaction is in the temporal dimension.
In this analysis, the same raw measured values were used to examine changes that occur over time.
That is, unlike the analysis presented in \cref{subsec:distributional_analysis}, here the order of the values plays a major role, and effects may be found in specific time windows.

To perform such an analysis over the entire interaction, two additional computation steps are required.
First, each point in time must have a corresponding value for each feature produced by all speakers.
This was achieved by smoothing the measured value using \ac{loess} \citep{Cleveland1988locally}, a non-parametric regression method that deterministically fits a function to a localized subset of the data.
The fitting was done for each speaker separately over all slices of \ac{hds}/\ac{dds} with measured values of the features.
This results in a predicted value for each slice of the conversation.
\cref{fig:condition_convergence_comparison} shows an example of these smoothed measures of one participant and Alexa for the \ac{f0} feature.
The lower part of the figure shows the accommodation changes of the participant during the interaction (blue for convergence and red for divergence), and the upper part shows the turn-taking events.
The confederate condition has fewer turn events, as the analysis concentrates on the participant and Alexa, and the confederate turns are not shown.
Secondly, the relationship between a feature's values in each slice needs to be determined to describe their temporal changes.
Since we are interested in accommodative behavior, a measure for the relative change between slices was used.
It calculates the participant's contribution in the overall change of distances between the participant and Alexa.
Alexa's contribution is considered as a static effect, as it is not able to change based on the user's speech input, and is therefore not taken into account.
The change tendency between two slices is calculated by
%
\begin{equation}
\label{eq:change}
change_t = -\Delta_{t, t-1} \mid S_{part} - S_{Alexa} \mid,
\end{equation}
\eqname{Smoothed change tendency of two interlocutors}
%
where the index $t$ refers to the current slice and $S_{part}$ and $S_{Alexa}$ are the smoothed values of the participant and Alexa, respectively.
The minus sign at the front flips the result so that increased proximity (convergence) is represented by positive values and distancing (divergence) by negative values (see \cref{fig:condition_convergence_comparison}).
Subsequently, the participant's contribution toward to accommodation is calculated by
%
\begin{equation}
\label{eq:accommodation}
accomm(participant)_t = change_t - \Delta_{t, t-1} S_{Alexa}.
\end{equation}
\eqname{Speaker's own contribution to mutual accommodation}
%
The sum of the proximity changes of each target produced by all participants in every sex-task-condition-order combination was calculated, resulting in a single value that represents the overall change.
A value greater than zero means that more convergence was observed, and a negative value points to more divergence.
There were only two instances where this value was exactly zero, both for the \ac{ar} feature.
These instances were treated as cases of divergence.
Using this approach, only a few interactions had no feature convergence in them, and several had all three features showing convergence.
However, we took a stricter approach, where a feature was considered as converging only if its overall accommodation value was higher than one standard deviation from its mean.
Based on that, all interactions were categorized by the number of features that showed more convergence in them.

\Cref{fig:alluvial} summarizes the categorization with respect to each factor individually.
Each line represents a single interaction.
The strata labels through which each line passes, indicate the group it belongs to with respect to each factor.
The number of features that showed more convergence than divergence overall are marked by the color of the line.
Some tendencies emerge from this categorization:
first, in \SI{35}{\percent} of the interactions, there was at least one feature that showed convergence, but in none of them did all three features do so.
In seven interactions, two features showed convergence, twice by males and five times by females.
In total, males converged in \SI{5}{\percent} of all measured features and females in \SI{7}{\percent}.
Furthermore, of all the instances of converged features, \SI{58}{\percent} occurred in the solo condition, compared to \SI{42}{\percent} in the confederate condition.
However, no difference between the Calendar and Quiz tasks was found, with \SI{49}{\percent} and \SI{51}{\percent} of the cases, respectively.
The same holds for the comparison between the two orders in which the tasks could be performed.

These results support the addition of the confederate to the interaction as the factor for less convergence.

\begin{figure}[t]
	\centering
	\includegraphics[width=\linewidth]{alluvial_4factors_numConv_strict}
	\caption
		[short caption]
		{Overview of the relation between the factors \emph{sex}, \emph{condition}, \emph{task}, and \emph{order} and the number of features that showed more convergence in total across the interactions.
		Each line represents one interaction.
		The \emph{sex} stratus refers to the sex of the participant, and the \emph{order} stratus to the position of an interaction in the order in which the task-condition combination was performed.
		The color of a line stands for the number of target features that showed overall convergence in this interaction, from none (zero features, in gray), through one (red), and up to two (blue).
		%		and up to all three features (green).
		For example, a blue line going through the strata sequence female $\rightarrow$ solo $\rightarrow$ Quiz $\rightarrow$ first represents an interaction with a female participant performing the Quiz task in solo condition first, where the participant converged in two out of the three target features.}
	\label{fig:alluvial}
\end{figure}

\section{Discussion and conclusion}
\label{sec:discussion_and_conclusion}

We have presented distributional and temporal analyses of differences in convergence of three vocal features in \ac{hci}, with emphasis on changes resulting from simultaneous, spoken \ac{hhi}.
The distributional analysis examined the difference in behavior of the features between interactions only with a computer-based device and interactions with both the device and a confederate, while the temporal analysis investigated the participants' account in the overall changes in proximity between the interlocutors.
To also consider the point in time during a session in which the confederate was speaking, the order factor was considered as well.
For all three features, it was found that
\begin{enumerate*}[(a)]
	\item the distributions differed more when the participant first interacted with the device alone, and
	\item more convergence was aggregated in the task that was performed first.
\end{enumerate*}
From these two findings, it can be concluded that chronological order of interactions affected the speech behavior of the participants.
Further analyses took the factors \emph{sex} and \emph{task} into account and indicated that female participants showed less convergence than male participants, but the task performed did not play any role in increasing the amount of convergence.
%Furthermore, as the task differ in their complexity and formality, this influences the convergence as well.
%The Calendar task is very formal and strict and thus the participants could use a very limited number of variability in their formulation.
%Furthermore, the role of the confederate speaker was similar to the role of Alexa, both had to give the same type of answers to the participant.
%Therefore, the presence of the confederate speaker does not influence the participants' speaking behavior that much.
%In the Quiz on the other hand, the participant and the confederate speaker build a team.
%Thus, their interaction is much more informal than the interaction with Alexa, which has the sole purpose of answering the questions.

The first speech input a participant encounters may cause a priming effect that, together with the natural tendency to converge to an interlocutor, results in more change in interactions that occur first.
However, the interchangeability of input (here, both \ac{hhi} and \ac{hci}) seems to hinder the ability of the participants to converge to Alexa.
One explanation for this may be that it is more natural for humans to accommodate to other humans, so once another human is involved, the accommodation towards the computer interlocutor is neglected.
Another possible explanation is that due to the multiple interlocutors, the participants do not have a steady target towards which to accommodate, which leads to a weakened convergence effect.
This is confirmed by the higher rate of convergence in the solo condition compared to the confederate condition.
Since \ac{hci} still lacks the mutuality of a comprehensive accommodation effect, the question arises whether these tendencies would be stronger in interactions with a single human versus interactions with two different human speakers simultaneously.
The higher number of convergence instances by females may be ascribed to the \ac{va} using a voice of the same sex and could be further investigated by using a \ac{va} with a male voice.
